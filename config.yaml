# data_config
dataset_mixer:
  AI-MO/NuminaMath-CoT: 0.0001
dataset_splits:
- train
- test
chat_template: "{% for message in messages %}{% if (message['role'] == 'system')%}{{ '' }}{% elif (message['role'] == 'user')%}{{ '### Problem: ' + message['content'] + '\n' }}{% elif (message['role'] == 'assistant')%}{{ '### Solution: ' + message['content'] + '\n' }}{% endif %}{% if loop.last and message['role'] == 'user' and add_generation_prompt %}{{ '### Solution: ' }}{% endif %}{% endfor %}"

# model_config
model_name_or_path: deepseek-ai/deepseek-math-7b-base
torch_dtype: float16
model_revision: main
trust_remote_code: true
attn_implementation: sdpa
use_peft: true
load_in_4bit: true
lora_target_modules: all-linear

# sft_config
output_dir: output

packing: true
max_seq_length: 2048
dataset_text_field: text

do_train: true
do_eval: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False

gradient_accumulation_steps: 1
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
num_train_epochs: 1

fp16: true
seed: 42

learning_rate: 2.0e-05
lr_scheduler_type: cosine
warmup_ratio: 0.

report_to: none
logging_strategy: steps
logging_steps: 1
log_level: passive
log_level_replica: passive

eval_strategy: steps
eval_steps: 0.249

push_to_hub: true
hub_private_repo: true
hub_strategy: every_save
hub_model_id: my_model

save_strategy: "no"

sdpa_kernel: mem