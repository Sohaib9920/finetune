# data_config
dataset_mixer:
  AI-MO/NuminaMath-CoT: 0.0001
dataset_splits:
- train
- test
chat_template: "{% for message in messages %}{% if (message['role'] == 'system')%}{{ '' }}{% elif (message['role'] == 'user')%}{{ '### Problem: ' + message['content'] + '\n' }}{% elif (message['role'] == 'assistant')%}{{ '### Solution: ' + message['content'] + '\n' }}{% endif %}{% if loop.last and message['role'] == 'user' and add_generation_prompt %}{{ '### Solution: ' }}{% endif %}{% endfor %}"

# model_config
model_name_or_path: deepseek-ai/deepseek-math-7b-base
torch_dtype: float16
model_revision: main
trust_remote_code: true
attn_implementation: sdpa
use_peft: true
load_in_8bit: true


# sft_config
output_dir: output
packing: true
dataset_text_field: text
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: True
do_train: true
do_eval: true
max_seq_length: 512
report_to: none
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
logging_steps: 20
eval_strategy: steps
num_train_epochs: 1
eval_steps: 20
