# data_config
dataset_mixer:
  AI-MO/NuminaMath-CoT: 0.0001
dataset_splits:
- train
- test
chat_template: "{% for message in messages %}{% if (message['role'] == 'system')%}{{ '' }}{% elif (message['role'] == 'user')%}{{ '### Problem: ' + message['content'] + '\n' }}{% elif (message['role'] == 'assistant')%}{{ '### Solution: ' + message['content'] + '\n' }}{% endif %}{% if loop.last and message['role'] == 'user' and add_generation_prompt %}{{ '### Solution: ' }}{% endif %}{% endfor %}"

# model_config
model_name_or_path: deepseek-ai/deepseek-math-7b-base
torch_dtype: bfloat16
model_revision: main
trust_remote_code: true
attn_implementation: sdpa
use_peft: true
load_in_4bit: true
lora_target_modules: all-linear

# sft_config
output_dir: output

packing: true
max_seq_length: 2048
dataset_text_field: text

do_train: true
do_eval: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False

gradient_accumulation_steps: 2
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
num_train_epochs: 1

fp16: false
seed: 42
bf16: false

learning_rate: 2.0e-05
lr_scheduler_type: cosine
warmup_ratio: 0.

report_to: wandb
logging_strategy: steps
logging_steps: 1
log_level: info
log_level_replica: warning

eval_strategy: steps
eval_steps: 0.24

push_to_hub: true
hub_private_repo: true
hub_strategy: every_save
hub_model_id: my_model_wandb_2

save_strategy: "no"

sdpa_kernel: mem

wandb_config:
  WANDB_PROJECT: "transformers"
  WANDB_RUN_GROUP: "group_00"
  WANDB_RUN_ID: "run_00"
  WANDB_TAGS: "sft" 